{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91d4ede1",
   "metadata": {},
   "source": [
    "# Assignment 1: Caption-Question Relevance Classification\n",
    "## Advanced Baselines for NLP\n",
    "\n",
    "**Task**: Binary classification to determine if a question is related to its corresponding caption\n",
    "\n",
    "**Assignment Requirements**:\n",
    "- âœ… Dataset: Min. 5,000 train / 1,000 test examples\n",
    "- âœ… Stratified train/test split with random_state=42\n",
    "- âœ… 5-Fold Stratified Cross-Validation\n",
    "- âœ… Primary Metric: F1-Macro\n",
    "- âœ… Feature Representations: TF-IDF (sparse) + Word Embeddings (dense)\n",
    "- âœ… Ablation Studies: N-grams, Preprocessing, Hyperparameters\n",
    "- âœ… Error Analysis: Confusion Matrix, Feature Analysis, Failure Cases\n",
    "\n",
    "**Methodology Improvements**:\n",
    "- ðŸŽ¯ **Semantic Distance-Based Negative Sampling**: Uses FastText embeddings of image tags to create semantically distant negative pairs, preventing false negatives\n",
    "- ðŸŽ¯ **Enhanced Context**: Concatenates Question + Answer for richer semantic features\n",
    "- ðŸŽ¯ **Tag-Based Embeddings**: Leverages image tags for robust semantic distance computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddafd1e",
   "metadata": {},
   "source": [
    "## âš ï¸ Prerequisites\n",
    "\n",
    "**Before running this notebook**, ensure you have:\n",
    "1. âœ… Run `convert_jsonl_to_parquet.ipynb` to generate parquet files **with tags included**\n",
    "2. âœ… The following files exist in your workspace:\n",
    "   - `RSVLM-QA-captions.parquet` (with columns: id, image, caption, **tags**)\n",
    "   - `RSVLM-QA-questions.parquet` (with columns: id, question_type, question, answer)\n",
    "\n",
    "This notebook uses the **tags** field to compute semantic distances for improved negative sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae944b",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff40583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP and text processing\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# Scikit-learn for ML\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score, \n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Word embeddings\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Set random seed for reproducibility (Assignment Requirement)\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef3394",
   "metadata": {},
   "source": [
    "## 2. Load Parquet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d52f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions data shape: (13820, 4)\n",
      "QA pairs data shape: (148558, 4)\n",
      "\n",
      "Captions columns: ['id', 'image', 'caption', 'tags']\n",
      "QA columns: ['id', 'question_type', 'question', 'answer']\n",
      "\n",
      "Number of unique images: 13820\n",
      "Average QA pairs per image: 10.75\n",
      "\n",
      "Tags available: True\n",
      "Average tags per image: 10.62\n"
     ]
    }
   ],
   "source": [
    "# Load the two parquet files\n",
    "df_captions = pd.read_parquet(\"RSVLM-QA-captions.parquet\")\n",
    "df_qa = pd.read_parquet(\"RSVLM-QA-questions.parquet\")\n",
    "\n",
    "print(f\"Captions data shape: {df_captions.shape}\")\n",
    "print(f\"QA pairs data shape: {df_qa.shape}\")\n",
    "print(f\"\\nCaptions columns: {df_captions.columns.tolist()}\")\n",
    "print(f\"QA columns: {df_qa.columns.tolist()}\")\n",
    "print(f\"\\nNumber of unique images: {df_captions['id'].nunique()}\")\n",
    "print(f\"Average QA pairs per image: {len(df_qa) / len(df_captions):.2f}\")\n",
    "print(f\"\\nTags available: {'tags' in df_captions.columns}\")\n",
    "if 'tags' in df_captions.columns:\n",
    "    print(f\"Average tags per image: {df_captions['tags'].apply(len).mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2bbe78",
   "metadata": {},
   "source": [
    "## 3. Create Binary Classification Dataset\n",
    "\n",
    "We'll create a balanced dataset with:\n",
    "- **Positive examples (label=1)**: Correct caption-question-answer triplets from the same image\n",
    "- **Negative examples (label=0)**: Semantically distant caption-question-answer pairs\n",
    "\n",
    "**Improvement**: Instead of random shuffling, we use tag-based semantic distance to pair captions with distant questions, preventing false negatives from similar content being paired together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666fdf1",
   "metadata": {},
   "source": [
    "### 3.1 Load FastText Embeddings for Tag Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba663e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FastText embeddings for tag vectorization...\n",
      "This may take a minute on first download...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading FastText embeddings for tag vectorization...\")\n",
    "print(\"This may take a minute on first download...\")\n",
    "\n",
    "try:\n",
    "    # Try to load FastText embeddings\n",
    "    fasttext_model = api.load('fasttext-wiki-news-subwords-300')\n",
    "    print(f\"âœ“ Loaded FastText embeddings: {len(fasttext_model)} words, {fasttext_model.vector_size} dimensions\")\n",
    "except Exception as e:\n",
    "    print(f\"! Unable to download FastText: {e}\")\n",
    "    print(\"! Using fallback: Loading smaller GloVe embeddings...\")\n",
    "    # Fallback to GloVe if FastText fails\n",
    "    fasttext_model = api.load('glove-wiki-gigaword-100')\n",
    "    print(f\"âœ“ Loaded GloVe embeddings: {len(fasttext_model)} words, {fasttext_model.vector_size} dimensions\")\n",
    "\n",
    "vector_size = fasttext_model.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c474dc23",
   "metadata": {},
   "source": [
    "### 3.2 Compute Tag Embeddings for Each Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48030500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_embedding(tags, model, vector_size):\n",
    "    \"\"\"\n",
    "    Convert a list of tags to an averaged embedding vector.\n",
    "    Each tag may consist of multiple words.\n",
    "    \"\"\"\n",
    "    if not tags or len(tags) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    tag_vectors = []\n",
    "    for tag in tags:\n",
    "        # Split multi-word tags and get embeddings for each word\n",
    "        words = str(tag).lower().replace('-', ' ').replace('_', ' ').split()\n",
    "        word_vecs = []\n",
    "        for word in words:\n",
    "            if word in model:\n",
    "                word_vecs.append(model[word])\n",
    "        \n",
    "        # Average words within the tag\n",
    "        if len(word_vecs) > 0:\n",
    "            tag_vectors.append(np.mean(word_vecs, axis=0))\n",
    "    \n",
    "    # Average all tag vectors\n",
    "    if len(tag_vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    return np.mean(tag_vectors, axis=0)\n",
    "\n",
    "print(\"Computing tag embeddings for all images...\")\n",
    "df_captions['tag_embedding'] = df_captions['tags'].apply(\n",
    "    lambda tags: get_tag_embedding(tags, fasttext_model, vector_size)\n",
    ")\n",
    "\n",
    "# Convert to numpy array for distance calculations\n",
    "tag_embeddings = np.vstack(df_captions['tag_embedding'].values)\n",
    "print(f\"âœ“ Tag embeddings shape: {tag_embeddings.shape}\")\n",
    "print(f\"âœ“ Sample tag embedding (first 5 dimensions): {tag_embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1b3755",
   "metadata": {},
   "source": [
    "### 3.3 Create Positive Examples (Correct Pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94476d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive examples created: 148558\n",
      "\n",
      "Sample positive example:\n",
      "Caption: The image depicts a highly developed urban area characterized by a prominent highway interchange that dominates the central portion of the scene. Surr...\n",
      "Question: Where is the highway interchange located in the image?\n",
      "Label: 1 (Related)\n"
     ]
    }
   ],
   "source": [
    "# Merge captions with QA pairs to create positive examples\n",
    "# Now concatenate question + answer for more context\n",
    "df_positive = df_qa.merge(df_captions[['id', 'caption']], on='id', how='inner')\n",
    "df_positive['question_answer'] = df_positive['question'] + ' ' + df_positive['answer']\n",
    "df_positive['label'] = 1  # Related caption-question pairs\n",
    "\n",
    "print(f\"Positive examples created: {len(df_positive)}\")\n",
    "print(f\"\\nSample positive example:\")\n",
    "sample = df_positive.iloc[0]\n",
    "print(f\"Caption: {sample['caption'][:150]}...\")\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Answer: {sample['answer'][:100]}...\")\n",
    "print(f\"Question+Answer: {sample['question_answer'][:150]}...\")\n",
    "print(f\"Label: {sample['label']} (Related)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2ad743",
   "metadata": {},
   "source": [
    "### 3.4 Create Negative Examples Using Semantic Distance\n",
    "\n",
    "Instead of random shuffling, we match each QA pair with captions from semantically distant images based on tag embeddings. This prevents false negatives where similar content is incorrectly labeled as unrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc2ba42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative examples created: 148438\n",
      "\n",
      "Sample negative example:\n",
      "Caption: The image predominantly features a cluster of multi-story residential buildings arranged in a grid-like pattern, surrounded by paved roads and parking...\n",
      "Question: Where is the highway interchange located in the image?\n",
      "Label: 0 (Unrelated)\n"
     ]
    }
   ],
   "source": [
    "# Create negative examples using semantic distance based on tag embeddings\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "print(\"Computing pairwise distances between tag embeddings...\")\n",
    "# Compute cosine distances between all tag embeddings\n",
    "distance_matrix = cosine_distances(tag_embeddings)\n",
    "print(f\"âœ“ Distance matrix shape: {distance_matrix.shape}\")\n",
    "\n",
    "# Create a mapping from id to index in the distance matrix\n",
    "id_to_idx = {id_val: idx for idx, id_val in enumerate(df_captions['id'].values)}\n",
    "\n",
    "print(\"\\nCreating negative examples by pairing with distant captions...\")\n",
    "df_negative = df_qa.copy()\n",
    "distant_captions = []\n",
    "\n",
    "for _, row in df_qa.iterrows():\n",
    "    original_id = row['id']\n",
    "    \n",
    "    if original_id in id_to_idx:\n",
    "        # Get the index of this image's embedding\n",
    "        orig_idx = id_to_idx[original_id]\n",
    "        \n",
    "        # Get distances to all other images\n",
    "        distances = distance_matrix[orig_idx]\n",
    "        \n",
    "        # Select from the top 25% most distant images to ensure diversity\n",
    "        # Sort by distance (descending) and pick randomly from top quartile\n",
    "        distant_indices = np.argsort(distances)[-len(distances)//4:]\n",
    "        \n",
    "        # Randomly select one from the distant images\n",
    "        selected_idx = np.random.choice(distant_indices)\n",
    "        distant_caption = df_captions.iloc[selected_idx]['caption']\n",
    "        distant_captions.append(distant_caption)\n",
    "    else:\n",
    "        # Fallback: random caption if ID not found\n",
    "        distant_captions.append(df_captions['caption'].sample(1, random_state=RANDOM_STATE).iloc[0])\n",
    "\n",
    "df_negative['caption'] = distant_captions\n",
    "df_negative['question_answer'] = df_negative['question'] + ' ' + df_negative['answer']\n",
    "df_negative['label'] = 0  # Unrelated caption-question pairs\n",
    "\n",
    "print(f\"âœ“ Negative examples created: {len(df_negative)}\")\n",
    "print(f\"\\nSample negative example (distant pairing):\")\n",
    "sample = df_negative.iloc[0]\n",
    "print(f\"Caption: {sample['caption'][:150]}...\")\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Answer: {sample['answer'][:100]}...\")\n",
    "print(f\"Label: {sample['label']} (Unrelated - Semantically Distant)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a76a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 296996\n",
      "Class distribution:\n",
      "label\n",
      "1    148558\n",
      "0    148438\n",
      "Name: count, dtype: int64\n",
      "Balance: label\n",
      "1    0.500202\n",
      "0    0.499798\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "âœ“ Dataset meets requirement: 296996 > 6,000 examples\n"
     ]
    }
   ],
   "source": [
    "# Combine positive and negative examples\n",
    "df_combined = pd.concat([df_positive, df_negative], ignore_index=True)\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "df_combined = df_combined.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "# Create combined text input: concatenate caption and question+answer\n",
    "df_combined['text'] = df_combined['caption'] + \" [SEP] \" + df_combined['question_answer']\n",
    "\n",
    "print(f\"Total dataset size: {len(df_combined)}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(df_combined['label'].value_counts())\n",
    "print(f\"Balance: {df_combined['label'].value_counts(normalize=True)}\")\n",
    "print(f\"\\nâœ“ Dataset meets requirement: {len(df_combined)} > 6,000 examples\")\n",
    "print(f\"âœ“ Using improved negative sampling: Semantically distant pairs\")\n",
    "print(f\"âœ“ Using enhanced context: Question + Answer concatenation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f79d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the semantic distances in negative examples\n",
    "print(\"Analyzing semantic distance in negative sampling...\")\n",
    "\n",
    "# Sample some negative examples and compute their distances\n",
    "sample_size = min(1000, len(df_negative))\n",
    "sample_negative = df_negative.head(sample_size)\n",
    "\n",
    "distances_in_negatives = []\n",
    "for idx, row in sample_negative.iterrows():\n",
    "    original_id = row['id']\n",
    "    if original_id in id_to_idx:\n",
    "        orig_idx = id_to_idx[original_id]\n",
    "        \n",
    "        # Find which caption was selected (match by text)\n",
    "        caption_matches = df_captions[df_captions['caption'] == row['caption']]\n",
    "        if len(caption_matches) > 0:\n",
    "            matched_id = caption_matches.iloc[0]['id']\n",
    "            if matched_id in id_to_idx:\n",
    "                matched_idx = id_to_idx[matched_id]\n",
    "                distance = distance_matrix[orig_idx, matched_idx]\n",
    "                distances_in_negatives.append(distance)\n",
    "\n",
    "if len(distances_in_negatives) > 0:\n",
    "    distances_in_negatives = np.array(distances_in_negatives)\n",
    "    \n",
    "    print(f\"\\nâœ“ Negative Pair Distance Statistics (n={len(distances_in_negatives)}):\")\n",
    "    print(f\"  Mean distance: {distances_in_negatives.mean():.4f}\")\n",
    "    print(f\"  Median distance: {np.median(distances_in_negatives):.4f}\")\n",
    "    print(f\"  Min distance: {distances_in_negatives.min():.4f}\")\n",
    "    print(f\"  Max distance: {distances_in_negatives.max():.4f}\")\n",
    "    print(f\"  Std deviation: {distances_in_negatives.std():.4f}\")\n",
    "    \n",
    "    # Compare to random sampling baseline\n",
    "    random_pairs = np.random.choice(len(df_captions), size=(sample_size, 2))\n",
    "    random_distances = [distance_matrix[i, j] for i, j in random_pairs if i != j]\n",
    "    random_distances = np.array(random_distances[:len(distances_in_negatives)])\n",
    "    \n",
    "    print(f\"\\n  Comparison to Random Pairing:\")\n",
    "    print(f\"  Random mean distance: {random_distances.mean():.4f}\")\n",
    "    print(f\"  Our method improvement: {((distances_in_negatives.mean() - random_distances.mean()) / random_distances.mean() * 100):+.1f}%\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(distances_in_negatives, bins=30, alpha=0.7, label='Semantic Distance Sampling', color='green')\n",
    "    plt.hist(random_distances, bins=30, alpha=0.7, label='Random Sampling', color='red')\n",
    "    plt.xlabel('Cosine Distance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Negative Sampling Strategy Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot([random_distances, distances_in_negatives], labels=['Random', 'Semantic Distance'])\n",
    "    plt.ylabel('Cosine Distance')\n",
    "    plt.title('Distance Distribution Comparison')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ“ Semantic distance-based sampling creates more distinct negative examples!\")\n",
    "else:\n",
    "    print(\"Could not compute distances for validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cd8e17",
   "metadata": {},
   "source": [
    "### 3.5 Validate Negative Sampling Strategy\n",
    "\n",
    "Verify that negative pairs are indeed semantically distant to avoid false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7190d0",
   "metadata": {},
   "source": [
    "## 4. Stratified Train/Test Split (Assignment Requirement)\n",
    "\n",
    "Split data with stratification to maintain class balance: 80% train, 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7c06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 237596 (80.0%)\n",
      "Test set size: 59400 (20.0%)\n",
      "\n",
      "Train class distribution:\n",
      "label\n",
      "1    118846\n",
      "0    118750\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test class distribution:\n",
      "label\n",
      "1    29712\n",
      "0    29688\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ“ Stratification verified: Both sets have balanced classes\n"
     ]
    }
   ],
   "source": [
    "# Stratified train/test split with random_state=42\n",
    "X = df_combined['text']\n",
    "y = df_combined['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=y  # Ensures class balance in both sets\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nTrain class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(y_test.value_counts())\n",
    "print(f\"\\nâœ“ Stratification verified: Both sets have balanced classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef16e58d",
   "metadata": {},
   "source": [
    "## 5. Text Preprocessing Strategies (Ablation Study)\n",
    "\n",
    "We'll implement different preprocessing levels to compare their impact on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b26fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Strategy Examples:\n",
      "Raw:        The highway interchange is located in the central portion! Where are the buildings?\n",
      "Lowercase:  the highway interchange is located in the central portion! where are the buildings?\n",
      "Clean:      the highway interchange is located in the central portion where are the buildings\n",
      "Aggressive: highway interchange located central portion buildings\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text, strategy='raw'):\n",
    "    \"\"\"\n",
    "    Apply different preprocessing strategies.\n",
    "    \n",
    "    Strategies:\n",
    "    - 'raw': No preprocessing\n",
    "    - 'lowercase': Convert to lowercase\n",
    "    - 'clean': Lowercase + remove punctuation\n",
    "    - 'aggressive': Lowercase + remove punctuation + remove stopwords\n",
    "    \"\"\"\n",
    "    # Handle None/NaN values\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string just in case\n",
    "    text = str(text)\n",
    "    \n",
    "    if strategy == 'raw':\n",
    "        return text\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    if strategy == 'lowercase':\n",
    "        return text\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    if strategy == 'clean':\n",
    "        return text\n",
    "    \n",
    "    # Remove stopwords (aggressive)\n",
    "    if strategy == 'aggressive':\n",
    "        from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "        words = text.split()\n",
    "        words = [w for w in words if w not in ENGLISH_STOP_WORDS]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test preprocessing strategies\n",
    "test_text = \"The highway interchange is located in the central portion! Where are the buildings?\"\n",
    "print(\"Preprocessing Strategy Examples:\")\n",
    "print(f\"Raw:        {preprocess_text(test_text, 'raw')}\")\n",
    "print(f\"Lowercase:  {preprocess_text(test_text, 'lowercase')}\")\n",
    "print(f\"Clean:      {preprocess_text(test_text, 'clean')}\")\n",
    "print(f\"Aggressive: {preprocess_text(test_text, 'aggressive')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7135d17d",
   "metadata": {},
   "source": [
    "## 6. Feature Representation: TF-IDF (Sparse Features)\n",
    "\n",
    "Experiment with different n-gram ranges (unigrams, bigrams, trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9671514d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Evaluation functions ready\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_cv(model, X_train_vec, y_train, cv=5):\n",
    "    \"\"\"\n",
    "    Evaluate model using Stratified K-Fold Cross-Validation.\n",
    "    Returns F1-Macro scores.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n",
    "    f1_scores = cross_val_score(model, X_train_vec, y_train, cv=skf, scoring='f1_macro')\n",
    "    return f1_scores\n",
    "\n",
    "# Storage for results\n",
    "results = []\n",
    "\n",
    "def run_experiment(name, vectorizer, model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Run a single experiment and store results.\"\"\"\n",
    "    # Vectorize\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Cross-validation on training set\n",
    "    cv_scores = evaluate_model_cv(model, X_train_vec, y_train, cv=5)\n",
    "    \n",
    "    # Train and test\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    \n",
    "    # Metrics\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'experiment': name,\n",
    "        'cv_f1_mean': cv_scores.mean(),\n",
    "        'cv_f1_std': cv_scores.std(),\n",
    "        'test_f1_macro': f1_macro,\n",
    "        'test_accuracy': accuracy\n",
    "    })\n",
    "    \n",
    "    print(f\"{name}\")\n",
    "    print(f\"  CV F1-Macro: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})\")\n",
    "    print(f\"  Test F1-Macro: {f1_macro:.4f}\")\n",
    "    print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    return model, vectorizer, y_pred\n",
    "\n",
    "print(\"âœ“ Evaluation functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fcdbab",
   "metadata": {},
   "source": [
    "### 6.1 Experiment: N-gram Comparison with TF-IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8ae38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPERIMENT 1: N-GRAM COMPARISON (TF-IDF)\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Apply lowercase preprocessing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m X_train_clean = \u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlowercase\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m X_test_clean = X_test.apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: preprocess_text(x, \u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Unigrams only\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/UPNA/NLP/A1_NLP/.venv/lib/python3.11/site-packages/pandas/core/series.py:5084\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4960\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4961\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4962\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4966\u001b[39m     **kwargs,\n\u001b[32m   4967\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4968\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4969\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4970\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5076\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   5077\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   5078\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5079\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5080\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5081\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5082\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5083\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m5084\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/UPNA/NLP/A1_NLP/.venv/lib/python3.11/site-packages/pandas/core/apply.py:1520\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1519\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/UPNA/NLP/A1_NLP/.venv/lib/python3.11/site-packages/pandas/core/apply.py:1578\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1577\u001b[39m     curried = func\n\u001b[32m-> \u001b[39m\u001b[32m1578\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1581\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1582\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1583\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/UPNA/NLP/A1_NLP/.venv/lib/python3.11/site-packages/pandas/core/base.py:1020\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action)\u001b[39m\n\u001b[32m   1017\u001b[39m arr = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m-> \u001b[39m\u001b[32m1020\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms.map_array(arr, mapper, na_action=na_action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/UPNA/NLP/A1_NLP/.venv/lib/python3.11/site-packages/pandas/core/arrays/arrow/array.py:1737\u001b[39m, in \u001b[36mArrowExtensionArray.map\u001b[39m\u001b[34m(self, mapper, na_action)\u001b[39m\n\u001b[32m   1731\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m map_array(\u001b[38;5;28mself\u001b[39m.to_numpy(), mapper, na_action=na_action)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1733\u001b[39m     \u001b[38;5;66;03m# For \"mM\" cases, the super() method passes `self` without the\u001b[39;00m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;66;03m#  to_numpy call, which inside map_array casts to ndarray[object].\u001b[39;00m\n\u001b[32m   1735\u001b[39m     \u001b[38;5;66;03m#  Without the to_numpy() call, NA is preserved instead of changed\u001b[39;00m\n\u001b[32m   1736\u001b[39m     \u001b[38;5;66;03m#  to None.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/UPNA/NLP/A1_NLP/.venv/lib/python3.11/site-packages/pandas/core/arrays/base.py:2692\u001b[39m, in \u001b[36mExtensionArray.map\u001b[39m\u001b[34m(self, mapper, na_action)\u001b[39m\n\u001b[32m   2672\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, mapper, na_action: Literal[\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2673\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2674\u001b[39m \u001b[33;03m    Map values using an input mapping or function.\u001b[39;00m\n\u001b[32m   2675\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2690\u001b[39m \u001b[33;03m        a MultiIndex will be returned.\u001b[39;00m\n\u001b[32m   2691\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2692\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/UPNA/NLP/A1_NLP/.venv/lib/python3.11/site-packages/pandas/core/algorithms.py:1710\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action)\u001b[39m\n\u001b[32m   1708\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1709\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1712\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(values, mapper, mask=isna(values).view(np.uint8))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:3071\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Apply lowercase preprocessing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m X_train_clean = X_train.apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlowercase\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      7\u001b[39m X_test_clean = X_test.apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: preprocess_text(x, \u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Unigrams only\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mpreprocess_text\u001b[39m\u001b[34m(text, strategy)\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Lowercase\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m text = \u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m()\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strategy == \u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[31mAttributeError\u001b[39m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 1: N-GRAM COMPARISON (TF-IDF)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Apply lowercase preprocessing\n",
    "X_train_clean = X_train.apply(lambda x: preprocess_text(x, 'lowercase'))\n",
    "X_test_clean = X_test.apply(lambda x: preprocess_text(x, 'lowercase'))\n",
    "\n",
    "# Unigrams only\n",
    "vec_unigram = TfidfVectorizer(ngram_range=(1, 1), max_features=5000)\n",
    "model_lr = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "run_experiment(\"TF-IDF Unigrams + LogReg\", vec_unigram, model_lr, \n",
    "               X_train_clean, X_test_clean, y_train, y_test)\n",
    "\n",
    "# Bigrams\n",
    "vec_bigram = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "model_lr = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "run_experiment(\"TF-IDF Bigrams + LogReg\", vec_bigram, model_lr, \n",
    "               X_train_clean, X_test_clean, y_train, y_test)\n",
    "\n",
    "# Trigrams\n",
    "vec_trigram = TfidfVectorizer(ngram_range=(1, 3), max_features=5000)\n",
    "model_lr = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "run_experiment(\"TF-IDF Trigrams + LogReg\", vec_trigram, model_lr, \n",
    "               X_train_clean, X_test_clean, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4465c",
   "metadata": {},
   "source": [
    "### 6.2 Experiment: Preprocessing Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1623276",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 2: PREPROCESSING ABLATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "preprocessing_strategies = ['raw', 'lowercase', 'clean', 'aggressive']\n",
    "\n",
    "for strategy in preprocessing_strategies:\n",
    "    X_train_prep = X_train.apply(lambda x: preprocess_text(x, strategy))\n",
    "    X_test_prep = X_test.apply(lambda x: preprocess_text(x, strategy))\n",
    "    \n",
    "    vec = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "    model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "    \n",
    "    run_experiment(f\"Preprocessing: {strategy}\", vec, model, \n",
    "                   X_train_prep, X_test_prep, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ea1330",
   "metadata": {},
   "source": [
    "### 6.3 Experiment: Model Comparison with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9017b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 3: MODEL COMPARISON (TF-IDF Features)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use best preprocessing from previous experiments\n",
    "X_train_prep = X_train.apply(lambda x: preprocess_text(x, 'lowercase'))\n",
    "X_test_prep = X_test.apply(lambda x: preprocess_text(x, 'lowercase'))\n",
    "\n",
    "# Logistic Regression\n",
    "vec = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "best_model, best_vec, y_pred_lr = run_experiment(\"Logistic Regression\", vec, model, \n",
    "                                                   X_train_prep, X_test_prep, y_train, y_test)\n",
    "\n",
    "# Naive Bayes\n",
    "vec = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "model = MultinomialNB()\n",
    "run_experiment(\"Naive Bayes\", vec, model, \n",
    "               X_train_prep, X_test_prep, y_train, y_test)\n",
    "\n",
    "# Random Forest\n",
    "vec = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "model = RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=100)\n",
    "run_experiment(\"Random Forest\", vec, model, \n",
    "               X_train_prep, X_test_prep, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d80de",
   "metadata": {},
   "source": [
    "### 6.4 Experiment: Hyperparameter Optimization (Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414bb160",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 4: HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Vectorize data\n",
    "vec = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "X_train_vec = vec.fit_transform(X_train_prep)\n",
    "X_test_vec = vec.transform(X_test_prep)\n",
    "\n",
    "# Grid search for Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "grid_lr = GridSearchCV(\n",
    "    LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    param_grid_lr,\n",
    "    cv=skf,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_lr.fit(X_train_vec, y_train)\n",
    "print(\"Logistic Regression - Best parameters:\", grid_lr.best_params_)\n",
    "print(f\"Logistic Regression - Best CV F1-Macro: {grid_lr.best_score_:.4f}\")\n",
    "\n",
    "y_pred_tuned = grid_lr.predict(X_test_vec)\n",
    "f1_tuned = f1_score(y_test, y_pred_tuned, average='macro')\n",
    "print(f\"Logistic Regression - Test F1-Macro: {f1_tuned:.4f}\\n\")\n",
    "\n",
    "results.append({\n",
    "    'experiment': 'Logistic Regression (Tuned)',\n",
    "    'cv_f1_mean': grid_lr.best_score_,\n",
    "    'cv_f1_std': 0,\n",
    "    'test_f1_macro': f1_tuned,\n",
    "    'test_accuracy': accuracy_score(y_test, y_pred_tuned)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2d3c6e",
   "metadata": {},
   "source": [
    "## 7. Feature Representation: Word Embeddings (Dense Features)\n",
    "\n",
    "Load pre-trained embeddings and create averaged word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25077856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(text, model, vector_size=100):\n",
    "    \"\"\"\n",
    "    Convert text to averaged word embeddings.\n",
    "    Returns a fixed-size vector by averaging word vectors.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    word_vecs = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            word_vecs.append(model[word])\n",
    "    \n",
    "    if len(word_vecs) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    return np.mean(word_vecs, axis=0)\n",
    "\n",
    "print(\"Loading pre-trained GloVe embeddings (this may take a minute)...\")\n",
    "print(\"Using glove-wiki-gigaword-100 (100-dimensional vectors)\")\n",
    "\n",
    "# Load GloVe embeddings\n",
    "try:\n",
    "    glove_model = api.load('glove-wiki-gigaword-100')\n",
    "    print(f\"âœ“ Loaded GloVe embeddings: {len(glove_model)} words, {glove_model.vector_size} dimensions\")\n",
    "except:\n",
    "    print(\"! Unable to download GloVe. Using fallback: training Word2Vec on our data...\")\n",
    "    # Fallback: Train Word2Vec on our data\n",
    "    sentences = [text.lower().split() for text in X_train]\n",
    "    glove_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4, seed=RANDOM_STATE)\n",
    "    glove_model = glove_model.wv\n",
    "    print(f\"âœ“ Trained Word2Vec: {len(glove_model)} words, {glove_model.vector_size} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 5: WORD EMBEDDINGS (Dense Features)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert text to word embeddings\n",
    "print(\"Converting texts to word embeddings...\")\n",
    "X_train_emb = np.array([get_word_vectors(text, glove_model, glove_model.vector_size) \n",
    "                        for text in X_train_prep])\n",
    "X_test_emb = np.array([get_word_vectors(text, glove_model, glove_model.vector_size) \n",
    "                       for text in X_test_prep])\n",
    "\n",
    "print(f\"Training embeddings shape: {X_train_emb.shape}\")\n",
    "print(f\"Test embeddings shape: {X_test_emb.shape}\\n\")\n",
    "\n",
    "# Logistic Regression with embeddings\n",
    "model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "cv_scores = cross_val_score(model, X_train_emb, y_train, cv=skf, scoring='f1_macro')\n",
    "\n",
    "model.fit(X_train_emb, y_train)\n",
    "y_pred_emb = model.predict(X_test_emb)\n",
    "f1_emb = f1_score(y_test, y_pred_emb, average='macro')\n",
    "acc_emb = accuracy_score(y_test, y_pred_emb)\n",
    "\n",
    "print(f\"Word Embeddings + LogReg\")\n",
    "print(f\"  CV F1-Macro: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})\")\n",
    "print(f\"  Test F1-Macro: {f1_emb:.4f}\")\n",
    "print(f\"  Test Accuracy: {acc_emb:.4f}\\n\")\n",
    "\n",
    "results.append({\n",
    "    'experiment': 'Word Embeddings + LogReg',\n",
    "    'cv_f1_mean': cv_scores.mean(),\n",
    "    'cv_f1_std': cv_scores.std(),\n",
    "    'test_f1_macro': f1_emb,\n",
    "    'test_accuracy': acc_emb\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ab5d8c",
   "metadata": {},
   "source": [
    "## 8. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results = df_results.sort_values('test_f1_macro', ascending=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ALL EXPERIMENT RESULTS (Sorted by Test F1-Macro)\")\n",
    "print(\"=\"*70)\n",
    "print(df_results.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Test F1-Macro comparison\n",
    "axes[0].barh(df_results['experiment'], df_results['test_f1_macro'], color='steelblue')\n",
    "axes[0].set_xlabel('Test F1-Macro Score')\n",
    "axes[0].set_title('Test F1-Macro Comparison')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# CV vs Test F1-Macro\n",
    "x = np.arange(len(df_results))\n",
    "width = 0.35\n",
    "axes[1].barh(x - width/2, df_results['cv_f1_mean'], width, label='CV F1-Macro', alpha=0.8)\n",
    "axes[1].barh(x + width/2, df_results['test_f1_macro'], width, label='Test F1-Macro', alpha=0.8)\n",
    "axes[1].set_yticks(x)\n",
    "axes[1].set_yticklabels(df_results['experiment'])\n",
    "axes[1].set_xlabel('F1-Macro Score')\n",
    "axes[1].set_title('Cross-Validation vs Test Performance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Best model: {df_results.iloc[0]['experiment']}\")\n",
    "print(f\"âœ“ Best Test F1-Macro: {df_results.iloc[0]['test_f1_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff54704",
   "metadata": {},
   "source": [
    "## 9. Error Analysis\n",
    "\n",
    "### 9.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac175b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for best model (Logistic Regression with TF-IDF)\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Unrelated', 'Related'])\n",
    "disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
    "ax.set_title('Confusion Matrix - Best Model (Logistic Regression + TF-IDF)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"=\"*70)\n",
    "print(\"CLASSIFICATION REPORT - Best Model\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(y_test, y_pred_lr, target_names=['Unrelated (0)', 'Related (1)'], digits=4))\n",
    "\n",
    "# Calculate per-class metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives (TN):  {tn:,}\")\n",
    "print(f\"  False Positives (FP): {fp:,}  (Unrelated classified as Related)\")\n",
    "print(f\"  False Negatives (FN): {fn:,}  (Related classified as Unrelated)\")\n",
    "print(f\"  True Positives (TP):  {tp:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d39d0f2",
   "metadata": {},
   "source": [
    "### 9.2 Discriminative Features Analysis\n",
    "\n",
    "Extract and analyze the most important features (words/n-grams) for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79108d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names and coefficients\n",
    "feature_names = best_vec.get_feature_names_out()\n",
    "coefficients = best_model.coef_[0]\n",
    "\n",
    "# Top features for RELATED class (positive coefficients)\n",
    "top_related_idx = np.argsort(coefficients)[-20:]\n",
    "top_related_features = [(feature_names[i], coefficients[i]) for i in top_related_idx]\n",
    "\n",
    "# Top features for UNRELATED class (negative coefficients)\n",
    "top_unrelated_idx = np.argsort(coefficients)[:20]\n",
    "top_unrelated_features = [(feature_names[i], coefficients[i]) for i in top_unrelated_idx]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TOP 20 DISCRIMINATIVE FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nMost Predictive of RELATED Caption-Question Pairs:\")\n",
    "print(\"-\" * 50)\n",
    "for feature, coef in reversed(top_related_features):\n",
    "    print(f\"  {feature:30s} : {coef:+.4f}\")\n",
    "\n",
    "print(\"\\n\\nMost Predictive of UNRELATED Caption-Question Pairs:\")\n",
    "print(\"-\" * 50)\n",
    "for feature, coef in top_unrelated_features:\n",
    "    print(f\"  {feature:30s} : {coef:+.4f}\")\n",
    "\n",
    "# Visualize top features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Related features\n",
    "related_words = [f[0] for f in reversed(top_related_features[-10:])]\n",
    "related_scores = [f[1] for f in reversed(top_related_features[-10:])]\n",
    "axes[0].barh(related_words, related_scores, color='green', alpha=0.7)\n",
    "axes[0].set_xlabel('Coefficient Value')\n",
    "axes[0].set_title('Top 10 Features for RELATED Pairs')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Unrelated features\n",
    "unrelated_words = [f[0] for f in top_unrelated_features[:10]]\n",
    "unrelated_scores = [f[1] for f in top_unrelated_features[:10]]\n",
    "axes[1].barh(unrelated_words, unrelated_scores, color='red', alpha=0.7)\n",
    "axes[1].set_xlabel('Coefficient Value')\n",
    "axes[1].set_title('Top 10 Features for UNRELATED Pairs')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce474f",
   "metadata": {},
   "source": [
    "### 9.3 Qualitative Failure Analysis\n",
    "\n",
    "Manual examination of misclassified examples to identify error patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c42f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get misclassified examples\n",
    "X_test_reset = X_test.reset_index(drop=True)\n",
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "\n",
    "misclassified = []\n",
    "for idx in range(len(y_test_reset)):\n",
    "    if y_pred_lr[idx] != y_test_reset.iloc[idx]:\n",
    "        misclassified.append({\n",
    "            'text': X_test_reset.iloc[idx],\n",
    "            'true_label': y_test_reset.iloc[idx],\n",
    "            'pred_label': y_pred_lr[idx]\n",
    "        })\n",
    "\n",
    "print(f\"Total misclassified examples: {len(misclassified)}\")\n",
    "print(f\"Error rate: {len(misclassified)/len(y_test)*100:.2f}%\\n\")\n",
    "\n",
    "# Categorize errors\n",
    "false_positives = [ex for ex in misclassified if ex['true_label'] == 0 and ex['pred_label'] == 1]\n",
    "false_negatives = [ex for ex in misclassified if ex['true_label'] == 1 and ex['pred_label'] == 0]\n",
    "\n",
    "print(f\"False Positives (predicted Related, actually Unrelated): {len(false_positives)}\")\n",
    "print(f\"False Negatives (predicted Unrelated, actually Related): {len(false_negatives)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"QUALITATIVE FAILURE ANALYSIS - 10 Sample Misclassifications\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show 5 false positives\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FALSE POSITIVES (Model predicted Related, but actually Unrelated)\")\n",
    "print(\"=\"*70)\n",
    "for i, ex in enumerate(false_positives[:5], 1):\n",
    "    parts = ex['text'].split(' [SEP] ')\n",
    "    caption = parts[0][:200] + \"...\" if len(parts[0]) > 200 else parts[0]\n",
    "    question = parts[1] if len(parts) > 1 else \"N/A\"\n",
    "    print(f\"\\n{i}. Caption: {caption}\")\n",
    "    print(f\"   Question: {question}\")\n",
    "    print(f\"   True: Unrelated | Predicted: Related\")\n",
    "    print(f\"   Error Type: Model incorrectly saw similarity\")\n",
    "\n",
    "# Show 5 false negatives\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"FALSE NEGATIVES (Model predicted Unrelated, but actually Related)\")\n",
    "print(\"=\"*70)\n",
    "for i, ex in enumerate(false_negatives[:5], 1):\n",
    "    parts = ex['text'].split(' [SEP] ')\n",
    "    caption = parts[0][:200] + \"...\" if len(parts[0]) > 200 else parts[0]\n",
    "    question = parts[1] if len(parts) > 1 else \"N/A\"\n",
    "    print(f\"\\n{i}. Caption: {caption}\")\n",
    "    print(f\"   Question: {question}\")\n",
    "    print(f\"   True: Related | Predicted: Unrelated\")\n",
    "    print(f\"   Error Type: Model failed to recognize relevance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a89579",
   "metadata": {},
   "source": [
    "## 10. Conclusions and Key Findings\n",
    "\n",
    "Summary of all experimental results and insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85531655",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ASSIGNMENT 1 - KEY FINDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ“Š DATASET:\")\n",
    "print(f\"  â€¢ Total examples: {len(df_combined):,}\")\n",
    "print(f\"  â€¢ Training set: {len(X_train):,} examples\")\n",
    "print(f\"  â€¢ Test set: {len(X_test):,} examples\")\n",
    "print(f\"  â€¢ Class balance: {df_combined['label'].value_counts(normalize=True)[1]:.1%} positive\")\n",
    "\n",
    "print(\"\\nðŸ”¬ METHODOLOGY IMPROVEMENTS:\")\n",
    "print(\"  âœ“ Semantic Distance-Based Negative Sampling:\")\n",
    "print(\"    - Uses FastText embeddings of image tags\")\n",
    "print(\"    - Pairs questions with semantically distant captions\")\n",
    "print(\"    - Prevents false negatives from similar content\")\n",
    "print(\"  âœ“ Enhanced Context:\")\n",
    "print(\"    - Concatenates Question + Answer for richer features\")\n",
    "print(\"    - Provides more semantic information for classification\")\n",
    "\n",
    "print(\"\\nðŸ† BEST PERFORMING MODEL:\")\n",
    "best_exp = df_results.iloc[0]\n",
    "print(f\"  â€¢ Model: {best_exp['experiment']}\")\n",
    "print(f\"  â€¢ Test F1-Macro: {best_exp['test_f1_macro']:.4f}\")\n",
    "print(f\"  â€¢ Test Accuracy: {best_exp['test_accuracy']:.4f}\")\n",
    "print(f\"  â€¢ CV F1-Macro: {best_exp['cv_f1_mean']:.4f} (Â±{best_exp['cv_f1_std']:.4f})\")\n",
    "\n",
    "print(\"\\nðŸ”¬ KEY EXPERIMENTAL INSIGHTS:\")\n",
    "print(\"\\n  1. N-GRAM COMPARISON:\")\n",
    "ngram_results = df_results[df_results['experiment'].str.contains('Unigrams|Bigrams|Trigrams')]\n",
    "if len(ngram_results) > 0:\n",
    "    best_ngram = ngram_results.iloc[0]\n",
    "    print(f\"     â€¢ Best n-gram strategy: {best_ngram['experiment']}\")\n",
    "    print(f\"     â€¢ F1-Macro: {best_ngram['test_f1_macro']:.4f}\")\n",
    "\n",
    "print(\"\\n  2. PREPROCESSING ABLATION:\")\n",
    "preproc_results = df_results[df_results['experiment'].str.contains('Preprocessing')]\n",
    "if len(preproc_results) > 0:\n",
    "    best_preproc = preproc_results.iloc[0]\n",
    "    print(f\"     â€¢ Best preprocessing: {best_preproc['experiment']}\")\n",
    "    print(f\"     â€¢ F1-Macro: {best_preproc['test_f1_macro']:.4f}\")\n",
    "\n",
    "print(\"\\n  3. FEATURE REPRESENTATION:\")\n",
    "tfidf_results = df_results[df_results['experiment'].str.contains('TF-IDF|LogReg')]\n",
    "emb_results = df_results[df_results['experiment'].str.contains('Embeddings')]\n",
    "if len(tfidf_results) > 0 and len(emb_results) > 0:\n",
    "    print(f\"     â€¢ TF-IDF (Sparse): {tfidf_results.iloc[0]['test_f1_macro']:.4f}\")\n",
    "    print(f\"     â€¢ Word Embeddings (Dense): {emb_results.iloc[0]['test_f1_macro']:.4f}\")\n",
    "    if tfidf_results.iloc[0]['test_f1_macro'] > emb_results.iloc[0]['test_f1_macro']:\n",
    "        print(f\"     â€¢ Winner: TF-IDF outperforms word embeddings\")\n",
    "    else:\n",
    "        print(f\"     â€¢ Winner: Word embeddings outperform TF-IDF\")\n",
    "\n",
    "print(\"\\n  4. MODEL COMPARISON:\")\n",
    "model_types = ['Logistic Regression', 'Naive Bayes', 'Random Forest']\n",
    "for model_type in model_types:\n",
    "    model_result = df_results[df_results['experiment'].str.contains(model_type)]\n",
    "    if len(model_result) > 0:\n",
    "        print(f\"     â€¢ {model_type}: {model_result.iloc[0]['test_f1_macro']:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… ASSIGNMENT REQUIREMENTS MET:\")\n",
    "print(\"  âœ“ Dataset size: > 6,000 examples (5,000 train + 1,000 test)\")\n",
    "print(\"  âœ“ Stratified train/test split with random_state=42\")\n",
    "print(\"  âœ“ 5-Fold Stratified Cross-Validation implemented\")\n",
    "print(\"  âœ“ Primary metric: F1-Macro used throughout\")\n",
    "print(\"  âœ“ Sparse features: TF-IDF implemented\")\n",
    "print(\"  âœ“ Dense features: Word Embeddings (GloVe/Word2Vec) implemented\")\n",
    "print(\"  âœ“ N-gram exploration: Unigrams, Bigrams, Trigrams compared\")\n",
    "print(\"  âœ“ Preprocessing ablation: Multiple strategies tested\")\n",
    "print(\"  âœ“ Hyperparameter optimization: Grid Search performed\")\n",
    "print(\"  âœ“ Error analysis: Confusion matrix + feature analysis + failure cases\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ ADVANCED METHODOLOGY CONTRIBUTIONS:\")\n",
    "print(\"  âœ“ Semantic distance-based negative sampling using tag embeddings\")\n",
    "print(\"  âœ“ FastText embeddings for robust tag vectorization\")\n",
    "print(\"  âœ“ Question-Answer concatenation for enhanced context\")\n",
    "print(\"  âœ“ Empirical validation of improved negative sampling strategy\")\n",
    "\n",
    "print(\"\\nðŸ“ RECOMMENDATIONS FOR FUTURE WORK:\")\n",
    "print(\"  â€¢ Experiment with more advanced models (SVM, XGBoost)\")\n",
    "print(\"  â€¢ Try contextual embeddings (BERT, RoBERTa)\")\n",
    "print(\"  â€¢ Implement attention mechanisms to weight important words\")\n",
    "print(\"  â€¢ Further tune semantic distance threshold for negative sampling\")\n",
    "print(\"  â€¢ Analyze question types separately for targeted improvements\")\n",
    "print(\"  â€¢ Explore ensemble methods combining TF-IDF and embeddings\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
